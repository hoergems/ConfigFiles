# General-purpose settings.
#color = false
verbose = true

[problem]
logPath = /datastore/hoe01h/WAFRExtension/emptyEnvironmentABTHFR/dubin/newReward/2
policyPath = pol.pol
environmentPath = environments/env_dubin_empty.xml
robotPath = models/test_4dof_old.urdf
robotConfigPath = config/dubin.cfg
goalStatesPath = goalstates_dubin.txt

overwriteExistingLogFiles = true

#################################################
## Uncertainty parameters
#################################################
minProcessCovariance = 2.5
maxProcessCovariance = 5.0

minObservationCovariance = 2.5
maxObservationCovariance = 5.0

incCovariance = process
covarianceSteps = 1

dynamicModel = lagrange
discountFactor = 0.999999

# discrete or continuous
actionType = discrete

# This has no effect when actionType is discrete
normalizeActions = false

exitReward = 1000.0
illegalMovePenalty = 500.0
illegalActionPenalty = 500.0
stepPenalty = 25.0
numInputSteps = 4

# The initial state of the robot. Has to be consistent with the number of degrees of freedom
initState = [-0.7, -0.7, 1.57, 0.0]

# linear or nonlinear
observationType = nonlinear

[DYNAMICS]
controlDuration = 0.3
simulationStepSize = 0.001
gravity = 9.81

[RRT]
rrtVerbose = false
planner = RRTConnect
planningVelocity = 2.0
continuousCollision = false
rrtGoalBias = 0.05

# This only affects ABT
continuousCollisionDynamic = false

# RRT timeout in milliseconds
rrtTimeout = 2000.0
numControlSamples = 1

[HFR]
controlSampler = continuous
dynamicPlanner = RRT
pathDeviationCost = 200
controlDeviationCost = 200
numEvaluationSamples = 250
minMaxControlDuration = [1, 10]
numThreads = 7

[MON]
#monThreshold = 0.5
monThreshold = 0.8

# Minimum number of sampled actions (0 => wait for timeout)
numSampledActions = 0

# The number os states that are sampled from the current belief for a sampled action
numSampledStates = 1

# The number of error that a sampled for each state-action pair
#numSampledErrors = 1000
numSampledErrors = 1000
numSampledErrorsObservation = 0

# Number of sampled states for the observation MoN
numSampledStatesObservation = 0

#Timeout to calculate the MoN in milliseconds
monTimeout = 10000

# Number of bins per dimension
#numBins = 15

# Ideal for SNM
numBins = 5

numMonThreads = 1
actionSamplingStrategy = discrete

# The number of sampled directions for the curvature-based measure
numSampledDirections = 1

[RANDOMSCENE]
randomScene = false
numRandomObstacles = 0
#randomObstacleDimensions = [0.032, 0.032, 0.05]
#intervalX = [-1, -0.15]
#intervalY = [-0.65, 0.05]
#intervalZ = [0.0, 0.0]
randomObstacleDimensions = [0.15, 0.15, 0.1]
intervalX = [-1.0, 1.0]
intervalY = [-1.0, 1.0]
intervalZ = [0.0, 0.0]

[changes]
hasChanges = false
changesPath = changes/changes-7-8.txt
areDynamic = true

[ABT]
# The number of trajectories to simulate per time step (0 => wait for timeout)
historiesPerStep = 100000

# The maximum time to spend on each step, in milliseconds (0 => no time limit)
stepTimeout = 1000.0

# If this is set to "true", ABT will prune the tree after every step.
pruneEveryStep = false

# If this is set to "true", ABT will reset the tree instead of modifying it when
# changes occur.
resetOnChanges = false

# The particle filter to use 'default' or 'observationModel'
particleFilter = observationModel

# The minimum number of particles for the current belief state in a simulation.
# Extra particles will be resampled via a particle filter if the particle count
# for the *current* belief state drops below this number during simulation.
minParticleCount = 3000

# Minimum number of effective particles below which resampling is happening
numEffectiveParticles = 30001

# Maximum number of particles tp plot
particlePlotLimit = 50

# Maximum time (in seconds) to replenish the particles
particleReplenishTimeout = 4.0

# The maximum depth to search in the tree, relative to the current belief.
maximumDepth = 500

maxObservationDistance = 0.2

# True if the above horizon is relative to the initial belief, and false
# if it's relative to the current belief.
isAbsoluteHorizon = false

searchHeuristic = default()

# For discrete action space
searchStrategy = ucb(5.0)

# For continuous action space
#searchStrategy = gps(searchType=compass, dimensions=2, explorationCoefficient=10, newSearchPointCoefficient=5, #minimumVisitsBeforeChildCreation=1, minimumChildCreationDistance=0.2, initialCompassRadiusRatio=1.0)
#recommendationStrategy = gpsmax(searchType=compass, dimensions=2, recommendationMode=mean)

estimator = max()

# Heuristic function to use (simple or dynamic)
heuristicFunction = distance

stateSamplingStrategy = default

numSamplesWeightedLazy = 10

[simulation]
loadInitialPolicy = false
savePolicy = false
saveParticles = true
nRuns = 100
nSteps = 150
showViewer = false
