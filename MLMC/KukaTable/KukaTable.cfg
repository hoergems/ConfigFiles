# General-purpose settings.
verbose = true
logPath = log
overwriteExistingLogFiles = true
logFilePostfix = correction1
saveParticles = false

scenePointMin = [-1, -1, -0.1]
scenePointMax = [0.5, 1, 1] 

##################################################################################################
##################################################################################################
#### Loaded plugins
##################################################################################################
##################################################################################################
[plugins]
#heuristicPlugin = libRRTHeuristicPlugin.so
heuristicPlugin = libKukaNNHeuristicPlugin.so

planningRewardPlugin = libdefaultRewardPlugin.so
executionRewardPlugin = libdefaultRewardPlugin.so

planningTerminalPlugin = libdefaultTerminalPlugin.so
executionTerminalPlugin = libdefaultTerminalPlugin.so

planningTransitionPlugin = libdefaultTransitionPlugin.so
executionTransitionPlugin = libdefaultTransitionPlugin.so

planningObservationPlugin = libdefaultObservationPlugin.so
executionObservationPlugin = libdefaultObservationPlugin.so

executionInitialBeliefPlugin = libdefaultInitialBeliefPlugin.so
planningInitialBeliefPlugin = libdefaultInitialBeliefPlugin.so

linearizerPlugin = libdefaultLinearizerPlugin.so

[transitionPluginOptions]
processError = 0.025
#processError = 0.0
controlDuration = 0.07
softLimitThreshold = 0.13

[observationPluginOptions]
observationError = 0.025

[rewardPluginOptions]
stepPenalty = 1
illegalMovePenalty = 500
exitReward = 1000

[heuristicPluginOptions]
planningRange = 0.03
goalState = [0.85, 1.5, 1.5, -0.85, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

[terminalPluginOptions]
goalLink = lwr_7_link
goalLinkPoint = [0, 0, 0]

[initialBeliefOptions]
lowerBound = [0.0, -1.5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
upperBound = [0.0, -1.5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]

[linearizerPluginOptions]
#epsProcess = 0.000001
epsProcess = 1e-11
epsObservation = 1e-11

##################################################################################################
##################################################################################################
#### Problem configuration
##################################################################################################
##################################################################################################
[problem]
# Number of simulation runs
nRuns = 1

# Maximum number of steps to reach the goal
nSteps = 40

policyPath = final-ABT-policy.pol

# The planning environment SDF
planningEnvironmentPath = kuka_table.sdf

# The execution environment SDF
executionEnvironmentPath = kuka_table.sdf

# The robot SDF model
robotPath = KukaModel.sdf

# Serialize the full world state (warning: logfiles can become huge)
enableGazeboStateLogging = false

# The discount factor of the reward model
discountFactor = 0.99

# Using state- action- and observation spaces that are normalized to [0, 1]
normalizedSpaces = true

allowCollisions = false

# The maximum time to spend on each step, in milliseconds (0 => no time limit)
stepTimeout = 5000

##################################################################################################
##################################################################################################
#### State, action and observation description
##################################################################################################
##################################################################################################
[state]
jointPositions = [lwr_a1_joint, lwr_a2_joint, lwr_e1_joint, lwr_a3_joint, lwr_a4_joint, lwr_a5_joint, lwr_a6_joint]
jointVelocities = [lwr_a1_joint, lwr_a2_joint, lwr_e1_joint, lwr_a3_joint, lwr_a4_joint, lwr_a5_joint, lwr_a6_joint]

[action]
jointTorques = [lwr_a1_joint, lwr_a2_joint, lwr_e1_joint, lwr_a3_joint, lwr_a4_joint, lwr_a5_joint, lwr_a6_joint]

[observation]
jointVelocities = [lwr_a1_joint, lwr_a2_joint, lwr_e1_joint, lwr_a3_joint, lwr_a4_joint, lwr_a5_joint, lwr_a6_joint]
linkPoses = [lwr_7_link]
linkPosesLowerLimits = [[-10, -10, -10, -5, -5, -5]]
linkPosesUpperLimits = [[10, 10, 10, 5, 5, 5]]

[options]
collisionInvariantLinks = [lwr_base_link]

##################################################################################################
##################################################################################################
#### ABT configuration
##################################################################################################
##################################################################################################
[ABT]
# The number of trajectories to simulate per time step (0 => wait for timeout)
historiesPerStep = 0

# If this is set to "true", ABT will prune the tree after every step.
pruneEveryStep = true

# If this is set to "true", ABT will reset the tree instead of modifying it when
# changes occur.
resetOnChanges = false

# The particle filter to use
particleFilter = observationModel

# The minimum number of particles for the current belief state.
minParticleCount = 200

# The maximum depth to search in the tree, relative to the current belief.
maximumDepth = 1000

# True if the above horizon is relative to the initial belief, and false
# if it's relative to the current belief.
isAbsoluteHorizon = false

searchHeuristic = default()
searchStrategy = ucb2(0.09)
#searchStrategy = approx(1.0, 10, 0.125, 100.0)

estimator = max()

recommendationStrategy = mean()

actionType = discrete
numInputStepsActions = 2

observationType = continuous
# The maximum L2-distance between observations for them to be considered similar
maxObservationDistance = 20000.0

# Maximum time (in seconds) to calculate a heuristc value
heuristicTimeout = 0.3

savePolicy = false
loadInitialPolicy = false
policyPath = final-0.pol

# Simulation step size that is used for planning (0.0 => same as execution)
planningSimulationStepSize = 0.02

#####################
### MLMC params
#####################
mlmc = true
maxLength = 3
numLevels = 4

keepPolicy = false

[DESPOT]
#numScenarios = 15
#searchDepth = 8
numScenarios = 5
searchDepth = 800
explorationConstant = 2.0
levelExplorationConstant = 0.005

[changes]
hasChanges = false
changesPath = changes/manipulator-changes.txt
areDynamic = true

[MHFR]
numEvaluationSamples = 100
controlSampler = discrete
dynamicPlanner = RRT
rrtTimeout = 20.0
rrtGoalBias = 0.0175
pathDeviationCost = 0.125
controlDeviationCost = 0.125
minMaxControlDuration = [1, 5]
numControlSamples = 1
numThreads = 6
allowApproximateSolutions = false

[SNM]
Threshold for SNM
#monThreshold = 0.4

# for neural network
#monThreshold = 0.525

monThreshold = 0.99

Threshold for NonGaussian
#monThreshold = 1.8

Threshold for Curvature
#monThreshold = 0.0

# Minimum number of sampled actions (0 => wait for timeout)
numSampledActions = 0

# The number of states that are sampled from the current belief for a sampled action
numSampledStates = 1

# The number of errors that are sampled for each state-action pair
numSampledErrorsTransition = 150
numSampledErrorsObservation = 50

# Number of sampled states for the observation MoN
numSampledStatesObservation = 50

#Timeout to calculate the MoN in milliseconds
monTimeout = 400
numBins = 2
#numBins = 2
numMonThreadsTransition = 7
numMonThreadsObservation = 0
actionSamplingStrategy = uniform

#The number of action samples after which the action distribution should be updated
sampleBatchSize = 50

[MABT]
maxNumInitialActions = 1
numInsertedActions = 1
maxNumActions = 10
deltaQThreshold = 0.9

[genTrajectorySamples]
minNumSamples = 20000
numThreads = 7
outputFile = trajectorySamplesKuka_998.txt
measureOutputFile = /home/marcus/PhD/scripts/oppt_devel/files/measure_KukaTable_998.txt
appendSamples = true

[genTrajectories]
minNumTrajectories = 20
numThreads = 7
outputFile = /home/marcus/PhD/scripts/oppt_devel/files/trajectoriesKukaTable/trajectories.txt
appendTrajectories = true

[NN]
test = false
numTrainingSamples = 0
numIterations = 100000
#architecture = [4, 10, 1]
architecture = [8, 15, 1]
trainingStrategy = QUASI_NEWTON_METHOD

# if rand below alpha => choose sample trajectory point
alpha = 0.7
filename = measure_out.txt
loadExistingSamples = true
expressionFile = expression_4DOF_SNM.txt

##################################################################################################
##################################################################################################
#### Simulation settings
##################################################################################################
##################################################################################################
[simulation]
interactive = false
particlePlotLimit = 80
