# General-purpose settings.
verbose = true
logPath = log
overwriteExistingLogFiles = true
logFilePostfix = 1
saveParticles = false

[plugins]
#heuristicPlugin = libRRTHeuristicPluginOld.so
heuristicPlugin = libdubinPassageNNHeuristicPlugin.so

planningRewardPlugin = libdefaultRewardPlugin.so
executionRewardPlugin = libdefaultRewardPlugin.so

planningTerminalPlugin = libdefaultTerminalPlugin.so
executionTerminalPlugin = libdefaultTerminalPlugin.so

planningTransitionPlugin = libdubinTransitionPlugin.so
executionTransitionPlugin = libdubinTransitionPlugin.so

planningObservationPlugin = libdubinObservationPlugin.so
#executionObservationPlugin = libdubinTrajectoryObservationPlugin.so
executionObservationPlugin = libdubinObservationPlugin.so

executionInitialBeliefPlugin = libdubinInitialBeliefPlugin.so
planningInitialBeliefPlugin = libdubinInitialBeliefPlugin.so

linearizerPlugin = libdefaultLinearizerPlugin.so

[transitionPluginOptions]
processError = 0.0195
controlDuration = 0.3

[observationPluginOptions]
observationError = 0.0195

[rewardPluginOptions]
stepPenalty = 1
illegalMovePenalty = 500
exitReward = 10000

[heuristicPluginOptions]
# For ABT
planningRange = 0.020

# For SNM-Planner
#planningRange = 0.055

goalState = [0.7, -0.7, 1.54, 0]

[terminalPluginOptions]
goalLink = DubinLink
goalLinkPoint = [0, 0, 0]

[initialBeliefOptions]
lowerBound = [-0.7, -0.7, 1.54, 0.0]
upperBound = [-0.7, -0.7, 1.54, 0.0]
#lowerBound = [0.0, 0.72, 0.0, 1.0]
#upperBound = [0.0, 0.78, 0.0, 1.0]

[linearizerPluginOptions]
epsProcess = 0.000001
epsObservation = 0.000001

[problem]
# Number of simulation runs
nRuns = 1

# Maximum number of steps to reach the goal
nSteps = 500

# The planning environment SDF
planningEnvironmentPath = DubinPlaneEnvironment.sdf

# The execution environment SDF
executionEnvironmentPath = DubinPlaneEnvironment.sdf

# The robot SDF model
robotPath = Dubin.sdf

enableGazeboStateLogging = false

# The discount factor of the reward model
discountFactor = 0.99

# Using state- action- and observation spaces that are normalized to [0, 1]
normalizedSpaces = true

allowCollisions = false

# The maximum time to spend on each step, in milliseconds (0 => no time limit)
stepTimeout = 1000

[state]
linkPositionsX = [DubinLink]
linkPositionsY = [DubinLink]
linkOrientationsZ = [DubinLink]
linkOrientationsZLimits = [[-50, 50]]

linkPositionXLimits = [[-1, 1]]
linkPositionYLimits = [[-1, 1]]

additionalDimensions = 1
additionalDimensionLimits = [[-0.2, 0.2]]

[action]
additionalDimensions = 2
additionalDimensionLimits = [[0.0, 1], [-1.0, 1.0]]

[observation]
additionalDimensions = 3
additionalDimensionLimits = [[-1.42, 1.42], [-1.42, 1.42], [0.0, 0.2]]

[changes]
hasChanges = false
changesPath = 
areDynamic = false

[ABT]
# The number of trajectories to simulate per time step (0 => wait for timeout)
historiesPerStep = 0

# If this is set to "true", ABT will prune the tree after every step.
pruneEveryStep = true

# If this is set to "true", ABT will reset the tree instead of modifying it when
# changes occur.
resetOnChanges = false

# The particle filter to use
particleFilter = observationModel

# The minimum number of particles for the current belief state in a simulation.
# Extra particles will be resampled via a particle filter if the particle count
# for the *current* belief state drops below this number during simulation.
minParticleCount = 2500
reuseCurrentParticles = false

# True if the above horizon is relative to the initial belief, and false
# if it's relative to the current belief.
isAbsoluteHorizon = false

searchStrategy = ucb2(2.0)

estimator = max()

recommendationStrategy = max()

heuristicTimeout = 0.1

savePolicy = false
loadInitialPolicy = false
policyPath = final-0.pol

actionType = discrete
numInputStepsActions = 4

observationType = continuous

# The maximum L2-distance between observations for them to be considered similar
#maxObservationDistance = 1.2
#maxObservationDistance = 0.085
maxObservationDistance = 0.1

observationPolicyUpdate = default

# Simulation step size that is used for planning (0.0 => same as execution)
planningSimulationStepSize = 0.8

#####################
### MLMC params
#####################
mlmc = true

# Max length of the correction histories (0 => infinite)
maxLength = 5
numLevels = 4
correctionLevels = [0.5, 0.3, 0.2, 0.1, 0.05]
#correctionLevels = [0.15, 0.125, 0.1, 0.075, 0.05]

keepPolicy = false
deleteSubtree = true
bellmanBackup = true
calcHeuristic = true

[DESPOT]
#numScenarios = 15
#searchDepth = 8
numScenarios = 5
searchDepth = 800
explorationConstant = 10.0
levelExplorationConstant = 0.005

[MABT]
maxNumInitialActions = 1
numInsertedActions = 5
maxNumActions = 10
deltaQThreshold = 0.95

[MHFR]
numEvaluationSamples = 100
controlSampler = continuous
dynamicPlanner = RRT
rrtTimeout = 10.0
rrtGoalBias = 0.1175
pathDeviationCost = 250
controlDeviationCost = 250
minMaxControlDuration = [1, 2]
numControlSamples = 1
numThreads = 7
allowApproximateSolutions = false

[SNM]
Threshold for SNM
monThreshold = 0.7

Threshold for NonGaussian
#monThreshold = 1.8

# Minimum number of sampled actions (0 => wait for timeout)
numSampledActions = 0

# The number of states that are sampled from the current belief for a sampled action
numSampledStates = 1

# The number of errors that are sampled for each state-action pair
numSampledErrorsTransition = 150
numSampledErrorsObservation = 150

#Timeout to calculate the MoN in milliseconds
monTimeout = 400
#numBins = 9
numBins = 3
numMonThreadsTransition = 6
numMonThreadsObservation = 6
actionSamplingStrategy = uniform

#The number of action samples after which the action distribution should be updated
sampleBatchSize = 50

snmSampleFile = measure_DubinMaze_out_30000.txt

[genTrajectories]
minNumTrajectories = 1500
numThreads = 8
outputFile = /home/marcus/PhD/scripts/oppt_devel/files/trajectoriesDubinMaze/trajectories.txt
appendTrajectories = true

[genTrajectorySamples]
minNumSamples = 2000
numThreads = 6
outputFile = /home/marcus/PhD/scripts/oppt_devel/files/trajectorySamplesDubin/trajectorySamplesDubinMazeTest.txt
measureOutputFileSNM = /home/marcus/PhD/scripts/oppt_devel/files/measureSamplesDubin/measureSamplesDubinMazeSNM5.txt
measureOutputFileMONG = /home/marcus/PhD/scripts/oppt_devel/files/measureSamplesDubin/measureSamplesDubinMazeMONG5.txt
measureOutputFileEMD = /home/marcus/PhD/scripts/oppt_devel/files/measureSamplesDubin/measureSamplesDubinMazeEMD4.txt
measureOutputFileTV = /home/marcus/PhD/scripts/oppt_devel/files/measureSamplesDubin/measureSamplesDubinMazeTV.txt
appendSamples = false
pomdpComponent = transition
measureUsed = TV
samplingMethod = RRT

[NN]
test = false
numTrainingSamples = 5000
numIterations = 100000

# 'uniform' or 'trajectory'
samplingStrategy = uniform
architecture = [2, 15, 15, 1]
#architecture = [8, 15, 1]
trainingStrategy = QUASI_NEWTON_METHOD

# if rand below alpha => choose sample trajectory point
alpha = 0.7
heuristicSamplesFilename = /home/marcus/PhD/scripts/oppt_devel/files/heuristicSamplesDubin/heuristicSamplesDubinPassage.txt
loadExistingSamples = true
expressionFile = expression_DubinPassage_heuristic.txt

[simulation]
lightweightSimulator = true
interactive = false
particlePlotLimit = 100
