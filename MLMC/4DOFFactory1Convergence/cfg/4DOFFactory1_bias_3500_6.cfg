# General-purpose settings.
verbose = true
logPath = /home/marcus/tt/4DOFFactory1/ 
overwriteExistingLogFiles = true
logFilePostfix = bias_3500_6 
saveParticles = true

[plugins]
#heuristicPlugin = libRRTHeuristicPlugin.so
#heuristicPlugin = libNearestNeighbourHeuristicPlugin.so
heuristicPlugin = lib4DOFNNHeuristicPlugin.so

planningRewardPlugin = libdefaultRewardPlugin.so
executionRewardPlugin = libdefaultRewardPlugin.so

planningTerminalPlugin = libdefaultTerminalPlugin.so
executionTerminalPlugin = libdefaultTerminalPlugin.so

planningTransitionPlugin = libdefaultTransitionPlugin.so
executionTransitionPlugin = libdefaultTransitionPlugin.so

planningObservationPlugin = libdefaultObservationPlugin.so
executionObservationPlugin = libdefaultObservationPlugin.so

executionInitialBeliefPlugin = libdefaultInitialBeliefPlugin.so
planningInitialBeliefPlugin = libdefaultInitialBeliefPlugin.so

linearizerPlugin = libdefaultLinearizerPlugin.so

[transitionPluginOptions]
processError = 0.05
controlDuration = 0.1
softLimitThreshold = 0.075

[observationPluginOptions]
observationError = 0.025

[rewardPluginOptions]
#stepPenalty = 0.001
#illegalMovePenalty = 0.5
#exitReward = 1.0
stepPenalty = 1.0
illegalMovePenalty = 500.0
exitReward = 1000.0

[heuristicPluginOptions]

# For ABT
planningRange = 0.06

# For SNM
#planningRange = 0.07
goalState = [1.57, 0, 0, 0, 0.0, 0.0, 0.0, 0.0]

[terminalPluginOptions]
goalLink = link4
goalLinkPoint = [1.0, 0, 0]

[initialBeliefOptions]
lowerBound = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
upperBound = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

[linearizerPluginOptions]
#epsProcess = 0.00000001
epsProcess = 5e-3
epsObservation = 0.00000001

[problem]
# Number of simulation runs
nRuns = 1

# Maximum number of steps to reach the goal
nSteps = 40

# The planning environment SDF
planningEnvironmentPath = 4DOFFactory1.sdf

# The execution environment SDF
executionEnvironmentPath = 4DOFFactory1.sdf

# The robot SDF model
robotPath = 4DOFManipulator.sdf

enableGazeboStateLogging = false

# The discount factor of the reward model
discountFactor = 0.98

# Using state- action- and observation spaces that are normalized to [0, 1]
normalizedSpaces = true

allowCollisions = false

# The maximum time to spend on each step, in milliseconds (0 => no time limit)
stepTimeout = 3500
[state]
jointPositions = [joint1, joint2, joint3, joint4]
jointVelocities = [joint1, joint2, joint3, joint4]

[action]
jointTorques = [joint1, joint2, joint3, joint4]

[observation]
jointPositions = [joint1, joint2, joint3, joint4]
#jointVelocities = [joint1, joint2, joint3, joint4]
linkPoses = [link4]
linkPosesLowerLimits = [[-5, -5, -5, -3.5, -3.5, -3.5]]
linkPosesUpperLimits = [[5, 5, 5, 3.5, 3.5, 3.5]]

[changes]
hasChanges = false
changesPath = 4DOFChanges.txt
areDynamic = false

[ABT]
# The number of trajectories to simulate per time step (0 => wait for timeout)
historiesPerStep = 0

# If this is set to "true", ABT will prune the tree after every step.
pruneEveryStep = true

# If this is set to "true", ABT will reset the tree instead of modifying it when
# changes occur.
resetOnChanges = false

# The particle filter to use
particleFilter = observationModel

# The minimum number of particles for the current belief state in a simulation.
# Extra particles will be resampled via a particle filter if the particle count
# for the *current* belief state drops below this number during simulation.
minParticleCount = 200

# True if the above horizon is relative to the initial belief, and false
# if it's relative to the current belief../abt --cfg ~/PhD/scripts/oppt_devel/cfg/4DOFFactory1
isAbsoluteHorizon = false

searchStrategy = ucb2(10.0)
#searchStrategy = hoeff(0.009, 0.2)
#searchStrategy = hoeff(0.9, 100.0)

######### For approximate dynamics #########
# explorationFactor, minNumSamples, maxDistanceToMean, maxCovariance
#searchStrategy = approx(0.009, 10, 0.175, 100.0)
############################################

estimator = max()

recommendationStrategy = max()

heuristicTimeout = 0.1

savePolicy = false
loadInitialPolicy = false
policyPath = final-0.pol

actionType = discrete
numInputStepsActions = 2
actionDiscretization = [2, 2, 2, 2]

observationType = continuous

# The maximum L2-distance between observations for them to be considered similar
#maxObservationDistance = 1.0
maxObservationDistance = 10000.36

# Defines how the policy is updated after a new observation
# 'default': do nothing
# 'splitting': update policy based on observation
observationPolicyUpdate = default

# Simulation step size that is used for planning (0.0 => same as execution)
planningSimulationStepSize = 0.02 

#####################
### MLMC params
#####################
mlmc = false 
maxLength = 3
numLevels = 5

[MABT]
maxNumInitialActions = 1
numInsertedActions = 5
maxNumActions = 16
#deltaQThreshold = 0.95
#deltaQThreshold = -0.0002
deltaQThreshold = -0.0002

[NN]
test = true

[MHFR]
numEvaluationSamples = 100
controlSampler = discrete
dynamicPlanner = RRT
rrtTimeout = 2.0
rrtGoalBias = 0.0575
pathDeviationCost = 250
controlDeviationCost = 250
minMaxControlDuration = [1, 3]
numControlSamples = 1
numThreads = 7
allowApproximateSolutions = false

[SNM]
Threshold for SNM
#monThreshold = 0.4

# for neural network
#monThreshold = 0.525

# For nearest neighbor
#monThreshold = 0.51
monThreshold = 1e10

Threshold for NonGaussian
#monThreshold = 1.8

Threshold for Curvature
#monThreshold = 0.0

# Minimum number of sampled actions (0 => wait for timeout)
numSampledActions = 0

# The number of states that are sampled from the current belief for a sampled action
numSampledStates = 1

# The number of errors that are sampled for each state-action pair
numSampledErrorsTransition = 250
numSampledErrorsObservation = 150000

# Number of sampled states for the observation MoN
numSampledStatesObservation = 50

#Timeout to calculate the MoN in milliseconds
monTimeout = 400
numBins = 2

numMonThreadsTransition = 1
numMonThreadsObservation = 1
actionSamplingStrategy = uniform

#The number of action samples after which the action distribution should be updated
sampleBatchSize = 50

[genTrajectorySamples]
minNumSamples = 50000
numThreads = 1
outputFile = /home/marcus/PhD/scripts/oppt_devel/files/transitionSamples4DOF/transitionSamples4DOF.txt
measureOutputFileSNM = /home/marcus/PhD/scripts/oppt_devel/files/measureSamples4DOFFactory1/measureSamples4DOFFactory1SNM5.txt
measureOutputFileMONG = /home/marcus/PhD/scripts/oppt_devel/files/measureSamples4DOFFactory1/measureSamples4DOFFactory1MONG5.txt
measureOutputFileEMD = /home/marcus/PhD/scripts/oppt_devel/files/measureSamples4DOFFactory1/measureSamples4DOFFactory1EMD.txt
measureOutputFileTV = /home/marcus/PhD/scripts/oppt_devel/files/measureSamples4DOF/measureSamples4DOFFactory1TVObservationNonAdditive.txt
appendSamples = false
pomdpComponent = observation
measureUsed = TV

[genTrajectories]
minNumTrajectories = 1500
numThreads = 7
outputFile = /home/marcus/PhD/scripts/oppt_devel/files/trajectories4DOFFactory1/trajectories_discrete.txt
appendTrajectories = true

[NN]
test = false
numTrainingSamples = 0
numIterations = 1000
#architecture = [4, 10, 1]
architecture = [12, 10, 10, 8]
trainingStrategy = QUASI_NEWTON_METHOD

# if rand below alpha => choose sample trajectory point
alpha = 0.7
filename = /home/marcus/PhD/scripts/oppt_devel/files/transitionSamples4DOF/transitionSamples4DOF.txt
loadExistingSamples = true
expressionFile = expression_4DOF_Transition.txt

[MultithreadedABT]
maxNumHistories = 20000

[simulation]
interactive = false
particlePlotLimit = 50
