# General-purpose settings.
verbose = true
logPath = /home/marcus/MovoTableT/MovoTable/ 
overwriteExistingLogFiles = true
logFilePostfix = correction_5 
saveParticles = false
deactivateVisualization = false

##################################################################################################
##################################################################################################
#### Loaded plugins
##################################################################################################
##################################################################################################
[plugins]
heuristicPlugin = libmovoGraspingHeuristicPlugin.so

planningRewardPlugin = libmovoGraspingRewardPlugin.so
executionRewardPlugin = libmovoGraspingRewardPlugin.so

planningTerminalPlugin = libmovoGraspingTerminalPlugin.so
executionTerminalPlugin = libmovoExecutionTerminalPlugin.so

planningTransitionPlugin = libmovoKinematicGraspingTransitionPlugin.so
executionTransitionPlugin = libmovoKinematicGraspingTransitionPlugin.so

planningObservationPlugin = libmovoObservationPlugin.so
executionObservationPlugin = libmovoObservationPlugin.so

executionInitialBeliefPlugin = libmovoInitialBeliefPlugin.so
planningInitialBeliefPlugin = libmovoInitialBeliefPlugin.so

[transitionPluginOptions]
#errorMargins = [0.0030, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030]

# In case uniform error distribution is used
errorMargins = [0.0, 0.0055, 0.0055, 0.0055, 0.0055, 0.0055, 0.0055]

jointCovariances = [0.0, 0.00006, 0.00006, 0.00006, 0.00006, 0.00006, 0.00006]
#jointCovariances = [0.0, 0.000019, 0.000019, 0.000019, 0.000019, 0.000019, 0.000019]
#jointCovariances = [0.0, 0.000015, 0.000015, 0.000015, 0.000015, 0.000015, 0.000015]

# Required for velocity control
controlDuration = 0.2
handLinkName = right_ee_link
objectDistanceCheckedLink = right_knuckle_link
gripperLinkName = right_wrist_3_link

#maximumGraspingDistance = 0.0075
maximumGraspingDistance = 0.00075

maximumGraspingZDistance = 0.045
maximumGraspingYDistance = 0.02
fingerClosingAngle = 1.0

# The time a step takes in the execution environment
stepDuration = 750
#stepDuration = 2000

# Probability that a grasp fails, despite all grasping conditions being met
failedGraspProbability = 0.01

[observationPluginOptions]
# The error margin of the uniform distribution of the object position
objectPositionErrorMargin = 0.035

# In case Gaussian error distribution is used
jointAngleCovariance = 0.00010

observationTopic = /popcorn/observation

[rewardPluginOptions]
stepPenalty = 3
illegalMovePenalty = 250

# Penalty when keeping the hand closed while no grasp is established
illegalHandClosedPenalty = 700.0

# Penalty when having a grasp and then opening the hand
illegalHandOpenPenalty = 3.0
failedGraspPenalty = 85
exitReward = 1000
graspReward = 1750
xRotationLimit = 0.28
yRotationLimit = 0.1
rotationPenalty = 100.0

yDistPenalty = 50.0

# Preferred motion direction inside the candy box
# 0=x, 1=y
preferredMotionDirection = 1

[heuristicPluginOptions]
planningRange = 0.03
goalState = [0.85, 1.5, 1.5, -0.85, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
maxHeuristicValue = 1000.0
heuristicScalingFactor = 8.0

enteringPose = [0.0, 1.0, -0.4]
exitPose = [0.0, 1.0, 0.05]

[terminalPluginOptions]
goalLink = lwr_7_link
goalLinkPoint = [0, 0, 0]
graspStateIsTerminal = false

[initialBeliefOptions]
# For new can
lowerBound = [0.866882, 0.2, 0.802, 0.0, 0.0, 0.0, 0.89017, -0.198839, 0.885714, -0.0372558, -0.00273644, 0.719018, 0.0, 0.0]
upperBound = [0.866882, 0.2, 0.802, 0.0, 0.0, 0.0, 0.89017, -0.198839, 0.885714, -0.0372558, -0.00273644, 0.719018, 0.0, 0.0]

lowerJointPositionErrorBound = [0.0, -0.01, -0.01, -0.01, -0.01, -0.01, -0.01]
upperJointPositionErrorBound = [0.0, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]
lowerObjectPositionErrorBound = [-0.00725, -0.00725, 0.0]
upperObjectPositionErrorBound = [0.00725, 0.00725, 0.0]
#lowerObjectPositionErrorBound = [-0.1725, -0.0725, 0.0]
#upperObjectPositionErrorBound = [0.1725, 0.0725, 0.0]

tableLinkName = box1

candyBoxLinkName = candyBox

linear_joint_angle = 0.14
pan_joint_angle = 0.0
tilt_joint_angle = 0.0
right_shoulder_pan_joint_angle = 0.0
right_shoulder_lift_joint_angle = -1.2
right_elbow_joint_angle = -1.0
right_wrist_1_joint_angle = -0.5
right_wrist_2_joint_angle = 0.0
right_wrist_3_joint_angle = -0.8
right_gripper_finger1_joint_angle = 0.0
right_gripper_finger2_joint_angle = 0.0
right_gripper_finger3_joint_angle = 0.0

##################################################################################################
##################################################################################################
#### Problem configuration
##################################################################################################
##################################################################################################
[problem]
# Number of simulation runs
nRuns = 30

# Maximum number of steps to reach the goal
nSteps = 200

policyPath = final-ABT-policy.pol

# The planning environment SDF
planningEnvironmentPath = MovoGripperOneArmTableEnvironment.sdf

# The execution environment SDF
executionEnvironmentPath = MovoGripperOneArmTableEnvironment.sdf

# The robot SDF model
robotPath = MovoGripperOneArm.sdf

# Serialize the full world state (warning: logfiles can become huge)
enableGazeboStateLogging = false

# The discount factor of the reward model
discountFactor = 1.0

# Using state- action- and observation spaces that are normalized to [0, 1]
normalizedSpaces = false

allowCollisions = true

# The maximum time to spend on each step, in milliseconds (0 => no time limit)
stepTimeout = 1000

[IK]
urdfFile = movo.urdf
baseLink = base_link
tipLink = right_ee_link

##################################################################################################
##################################################################################################
#### State, action and observation description
##################################################################################################
##################################################################################################
[state]
# First 6 dimensions: Object pose
linkPoses = [cylinderLink]

# Next 6 dimensions: Hand pose
# 13th dimension: hand open/closed
# 14th dimension: contact establised/not established
# 15th dimension: localized/not localized
additionalDimensions = 8
additionalDimensionLimits = [[-10, 10], [-10, 10], [-10, 10], [-10, 10], [-10, 10], [-10, 10], [0, 1], [0, 1]]

[action]
# First six dimensions are the joint position increments (in degrees)
# or velocity increments (in degress/s)
# Last dimension is opening/closing motion
additionalDimensions = 8
additionalDimensionLimits = [[-0.075, 0.075], [-0.075, 0.075], [-0.075, 0.075], [-0.075, 0.075], [-0.1, 0.1], [-0.075, 0.075], [-1, 1], [0, 1]]

# Use for verlocity control mode
#additionalDimensionLimits = [[-0.2, 0.2], [-0.2, 0.2], [-0.2, 0.2], [-0.2, 0.2], [-0.2, 0.2], [-0.2, 0.2], [-1, 1]]

[observation]
ignoreGazeboSensors = true
# Object pose, hand pose, finger open/closed, graspEstablished
additionalDimensions = 10
#additionalDimensionLimits = [[0, 1], [0, 1]]

# Object pose, hand pose, finger open/closed, graspEstablished
additionalDimensions = 14
#additionalDimensionLimits = [[0, 1], [0, 1]]

[options]
#collisionInvariantLinks = [base_link, cylinderLink, right_gripper_finger2_knuckle_link, linear_actuator_link, pan_link, right_forearm_link, right_wrist_3_link, tilt_link, right_shoulder_link, right_gripper_finger3_knuckle_link, right_wrist_2_link, right_wrist_1_link, right_gripper_finger1_knuckle_link]
collisionInvariantLinks = [base_link, pan_link, tilt_link, linear_actuator_link]

##################################################################################################
##################################################################################################
#### ABT configuration
##################################################################################################
##################################################################################################
[ABT]
# The number of trajectories to simulate per time step (0 => wait for timeout)
historiesPerStep = 0

# If this is set to "true", ABT will prune the tree after every step.
pruneEveryStep = true

# If this is set to "true", ABT will reset the tree instead of modifying it when
# changes occur.
resetOnChanges = false

# The particle filter to use
particleFilter = observationModel

# The minimum number of particles for the current belief state.
minParticleCount = 1500

# Allow zero weight particle to be part of the next belief
allowZeroWeightParticles = false

# The maximum depth to search in the tree, relative to the current belief.
maximumDepth = 1000

# True if the above horizon is relative to the initial belief, and false
# if it's relative to the current belief.
isAbsoluteHorizon = false

searchHeuristic = default()
searchStrategy = ucb2(3.0)

estimator = max()
recommendationStrategy = max()

actionType = discrete
numInputStepsActions = 2
actionDiscretization = [2, 2, 2, 2, 2, 2, 3, 1]

observationType = continuous
# The maximum L2-distance between observations for them to be considered similar
#maxObservationDistance = 20.3
maxObservationDistance = 0.07

observationSplittingMechanism = distance

# Maximum time (in seconds) to calculate a heuristc value
heuristicTimeout = 0.3

savePolicy = false
loadInitialPolicy = false
policyPath = final-0.pol

# Defines how the policy is updated after a new observation
# 'default': do nothing
# 'splitting': update policy based on observation
observationPolicyUpdate = default

# Simulation step size that is used for planning (0.0 => same as execution)
planningSimulationStepSize = 0.0

#####################
### MLMC params
#####################
mlmc = true 

# Max length of the correction histories (0 => infinite)
maxLength = 2
numLevels = 2
correctionLevels = [1.25, 1.0, 0.75, 0.5, 0.25]
keepPolicy = false

[DESPOT]
#numScenarios = 15
#searchDepth = 8
numScenarios = 5
searchDepth = 800
explorationConstant = 10.0

[MultithreadedABT]
maxNumHistories = 10000
singleScanAction = true

[CalcIK]
numSamples = 500
outputFileObject = /home/marcus/PhD/scripts/oppt_devel/files/IKSamples/samplesObject.txt
outputFileRetract = /home/marcus/PhD/scripts/oppt_devel/files/IKSamples/samplesRetract.txt
lowerDistrBoundsObject = [-0.3, -0.3]
upperDistrBoundsObject = [0.3, 0.3]
lowerDistrBoundsRetract = [0.8, -0.2]
upperDistrBoundsRetract = [0.95, -0.15]
# z-position increase from the intial pose for the target pose
retractZPositionInc = 0.01
minDistanceFromObject = 0.1
maxDistanceFromObject = 0.12
retractMode = false

[changes]
hasChanges = false
changesPath = changes/manipulator-changes.txt
areDynamic = true

##################################################################################################
##################################################################################################
#### Simulation settings
##################################################################################################
##################################################################################################
[simulation]
interactive = false
particlePlotLimit = 5
lightweightSimulator = false
