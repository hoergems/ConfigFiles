# General-purpose settings.
verbose = true

[plugins]
rewardPlugin = libdefaultRewardPlugin.so
rewardOptionsConfig = DefaultRewardFunction.cfg

heuristicPlugin = libRRTHeuristicPlugin.so
heuristicOptionsConfig = RRTHeuristic.cfg

planningPropagatorPlugin = libgazeboPropagatorPlugin.so
executionPropagatorPlugin = libgazeboPropagatorPlugin.so

planningObservationPlugin = libgazeboObservationPlugin.so
executionObservationPlugin = libgazeboObservationPlugin.so

[problem]
logPath = /home/marcus/PhD/testFOld/res/4DOFFactory1/3_proc_1_obs/ 
logFilePostfix = 48 
policyPath = final-0.pol

# The planning environment SDF
planningEnvironmentPath = 4DOFFactory1.sdf

# The execution environment SDF
executionEnvironmentPath = 4DOFFactory1.sdf

# The robot SDF model
robotPath = 4DOFManipulator.sdf

# The robot configuration file
robotConfigPath = 4DOFManipulator.cfg

enableGazeboStateLogging = true
overwriteExistingLogFiles = true

# Uncertainty parameters
processError = 0.038 
observationError = 0.001 

# The discount factor of the reward model
discountFactor = 0.99

# 'discrete' or 'continuous'
actionType = discrete

# Using state- action- and observation spaces that are normalized to [0, 1]
normalizedSpaces = true

numInputSteps = 2

allowCollisions = false

# The maximum time to spend on each step, in milliseconds (0 => no time limit)
stepTimeout = 2000

[dynamics]
controlDuration = 0.1
softLimitThreshold = 0.03

[changes]
hasChanges = false
changesPath = 4DOFChanges.txt
areDynamic = false

[ABT]
# The number of trajectories to simulate per time step (0 => wait for timeout)
historiesPerStep = 0

# If this is set to "true", ABT will prune the tree after every step.
pruneEveryStep = true

# If this is set to "true", ABT will reset the tree instead of modifying it when
# changes occur.
resetOnChanges = false

# The particle filter to use
particleFilter = observationModel

# The minimum number of particles for the current belief state in a simulation.
# Extra particles will be resampled via a particle filter if the particle count
# for the *current* belief state drops below this number during simulation.
minParticleCount = 3000

# Maximum number of particles to plot
particlePlotLimit = 30

# The maximum depth to search in the tree, relative to the current belief.
maximumDepth = 1000

# The maximum L2-distance between observations for them to be considered similar
maxObservationDistance = 2.0

# True if the above horizon is relative to the initial belief, and false
# if it's relative to the current belief.
isAbsoluteHorizon = false

searchHeuristic = default()
searchStrategy = ucb(2.0)

# explorationFactor, minNumSamples, maxDistanceToMean, maxCovariance
#searchStrategy = approx(2.0, 10, 0.525, 0.005)
estimator = max()

############################# For continuous actions #############################
#searchStrategy = gps2(searchType=compass, dimensions=0, explorationCoefficient=10, newSearchPointCoefficient=5, minimumVisitsBeforeChildCreation=1, minimumChildCreationDistance=0.2, #initialCompassRadiusRatio=1.0)
#recommendationStrategy = gps2max(searchType=compass, dimensions=0, recommendationMode=mean)
##################################################################################

heuristicTimeout = 0.1

[MHFR]
numEvaluationSamples = 100
controlSampler = discrete
dynamicPlanner = RRT
rrtTimeout = 100000.0
rrtGoalBias = 0.0175
pathDeviationCost = 1000
controlDeviationCost = 1000
minMaxControlDuration = [1, 5]
numControlSamples = 1
numThreads = 7
epsProcess = 0.0000000001
epsObservation = 0.0000000001

[SNM]
Threshold for SNM
#monThreshold = 0.6
monThreshold = 100000

Threshold for NonGaussian
#monThreshold = 1.8

Threshold for Curvature
#monThreshold = 0.0

# Minimum number of sampled actions (0 => wait for timeout)
numSampledActions = 0

# The number of states that are sampled from the current belief for a sampled action
numSampledStates = 1

# The number of errors that are sampled for each state-action pair
numSampledErrors = 75
numSampledErrorsObservation = 50

# Number of sampled states for the observation MoN
numSampledStatesObservation = 50

#Timeout to calculate the MoN in milliseconds
monTimeout = 5000
numBins = 3
numMonThreads = 17
actionSamplingStrategy = uniform

#The number of action samples after which the action distribution should be updated
sampleBatchSize = 50

# The number of sampled directions for the curvature-based measure
numSampledDirections = 1

[simulation]
interactive = false
loadInitialPolicy = false
savePolicy = false
saveParticles = true
nRuns = 1
nSteps = 50
showViewer = false
